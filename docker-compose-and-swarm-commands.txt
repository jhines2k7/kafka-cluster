docker run \
--net=kafkanet \
--rm \
confluentinc/cp-kafka:4.0.0 \
kafka-topics --create --topic topic1 --partitions 3 --replication-factor 3 --if-not-exists --zookeeper zk1:22181,zk2:32181,zk3:42181

docker run \
--net=kafkanet \
--rm \
confluentinc/cp-kafka:4.0.0 \
kafka-topics --describe --topic topic1 --zookeeper zk1:22181,zk2:32181,zk3:42181

docker run \
--net=kafkanet \
--rm confluentinc/cp-kafka:4.0.0 \
bash -c "seq 42 | kafka-console-producer --broker-list kafka1:29092,kafka2:39092,kafka3:49092 --topic topic1 && echo 'Produced 42 messages.'"

docker run \
--net=kafkanet \
--rm \
confluentinc/cp-kafka:4.0.0 \
kafka-console-consumer --bootstrap-server kafka1:29092 --topic topic1 --new-consumer --from-beginning --max-messages 42

docker run -it --net=kafkanet --rm confluentinc/cp-schema-registry:4.0.0 bash

/usr/bin/kafka-avro-console-producer --broker-list kafka1:29092,kafka2:39092,kafka3:39092 --topic schemademo --property schema.registry.url=http://schemaregistry:8081 --property value.schema='{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'

{"f1": "value1"}
{"f1": "value2"}
{"f1": "value3"}

docker run -it --net=kafkanet --rm confluentinc/cp-schema-registry:4.0.0 bash

curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
--data '{"name": "my_consumer_instance", "format": "avro", "auto.offset.reset": "smallest"}' \
http://kafkarest:8082/consumers/my_avro_consumer

curl -X GET -H "Accept: application/vnd.kafka.avro.v1+json" \
http://kafkarest:8082/consumers/my_avro_consumer/instances/my_consumer_instance/topics/schemademo

curl http://kafkarest:8082/topics

docker run \
--net=kafkanet \
--rm \
confluentinc/cp-kafka:4.0.0 \
kafka-topics --create --topic connect-config --partitions 3 --replication-factor 3 --if-not-exists --zookeeper zk1:22181,zk2:32181,zk3:42181

docker run \
--net=kafkanet \
--rm \
confluentinc/cp-kafka:4.0.0 \
kafka-topics --create --topic connect-offsets --partitions 3 --replication-factor 3 --if-not-exists --zookeeper zk1:22181,zk2:32181,zk3:42181

docker run \
--net=kafkanet \
--rm \
confluentinc/cp-kafka:4.0.0 \
kafka-topics --create --topic connect-data --partitions 3 --replication-factor 3 --if-not-exists --zookeeper zk1:22181,zk2:32181,zk3:42181

docker run \
--net=kafkanet \
--rm confluentinc/cp-kafka:4.0.0 \
kafka-topics --create --topic c3-test --partitions 3 --replication-factor 3 --if-not-exists --zookeeper zk1:22181,zk2:32181,zk3:42181

docker run \
--net=kafkanet \
--rm confluentinc/cp-kafka:4.0.0 \
kafka-topics --create --topic quickstart-data --partitions 3 --replication-factor 3 --if-not-exists --zookeeper zk1:22181,zk2:32181,zk3:42181

while true;
do
    docker run \
    --net=kafkanet \
    --rm \
    -e CLASSPATH=/usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar \
    confluentinc/cp-kafka-connect:4.0.0 \
    bash -c 'seq 10000 | kafka-console-producer --request-required-acks 1 --broker-list kafka1:29092,kafka2:39092,kafka3:49092 --topic c3-test --producer-property interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor --producer-property acks=1 && echo "Produced 10000 messages."'
    sleep 10;
done

OFFSET=0

while true;
do
    docker run \
    --net=kafkanet \
    --rm \
    -e CLASSPATH=/usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar \
    confluentinc/cp-kafka-connect:4.0.0 \
    bash -c 'kafka-console-consumer --consumer-property group.id=qs-consumer --consumer-property interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor --bootstrap-server kafka1:29092,kafka2:39092,kafka3:49092 --topic c3-test --offset '$OFFSET' --partition 0 --max-messages=1000'
    sleep 1;
    let OFFSET=OFFSET+1000
done

# create source connector
docker exec e55553f693a4 curl -s -X POST \
-H "Content-Type: application/json" \
--data '{"name": "quickstart-file-source", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector", "tasks.max":"1", "topic":"quickstart-data", "file": "/tmp/quickstart/file/input.txt"}}' \
http://kafkaconnect:8083/connectors

docker exec e55553f693a4 curl -s -X GET http://kafkaconnect:8083/connectors/quickstart-file-source/status

docker run \
--net=kafkanet \
--rm \
confluentinc/cp-kafka:4.0.0 \
kafka-console-consumer --bootstrap-server kafka1:29092,kafka2:39092,kafka3:49092 --topic quickstart-data --from-beginning

# create sink connector
docker exec e55553f693a4 curl -X POST -H "Content-Type: application/json" \
--data '{"name": "quickstart-file-sink", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector", "tasks.max":"1", "topics":"quickstart-data", "file": "/tmp/quickstart/file/output.txt"}}' \
http://kafkaconnect:8083/connectors

docker exec e55553f693a4 sh -c 'seq 50 >> /tmp/quickstart/file/input.txt'
docker exec e55553f693a4 cat /tmp/quickstart/file/output.txt
docker exec e55553f693a4 cat /tmp/quickstart/file/input.txt
docker exec e55553f693a4 cat /tmp/quickstart/file/input.txt